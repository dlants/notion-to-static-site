<!DOCTYPE html>
<html>
<head>
  <style>button,hr,input{overflow:visible}audio,canvas,progress,video{display:inline-block}progress,sub,sup{vertical-align:baseline}html{font-family:sans-serif;line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0} menu,article,aside,details,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}sup{top:-.5em}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{}button,select{text-transform:none}[type=submit], [type=reset],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:ButtonText dotted 1px}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}[hidden],template{display:none}.mention_f1qbn7vm a,.childPage_f1qbn7vm a{color:hsl(0, 0%, 0%);font-weight:bold;text-decoration-color:hsl(0, 0%, 90%)}.mention_f1qbn7vm a:hover,.childPage_f1qbn7vm a:hover{background-color:hsl(0, 0%, 90%)}.video_f1585p9t{-ms-flex-direction:column;-webkit-flex-direction:column;align-items:center;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-direction:column;width:100%}.video_f1585p9t iframe{-ms-flex-negative:0;-webkit-flex-shrink:0;flex-basis:auto;flex-shrink:0}.paragraph_f130nt8a{min-height:1em;padding-bottom:3px;padding-top:3px}.quote_f1rkq6po{border-left:hsl(0, 0%, 40%) solid 2px;padding-left:20px}.image_f1xdg44s{-ms-flex-direction:column;-webkit-flex-direction:column;align-items:center;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-direction:column;width:100%}.image_f1xdg44s img{-ms-flex-negative:0;-webkit-flex-shrink:0;flex-basis:auto;flex-shrink:0;max-width:100%}.imageCaption_f1jz7e68{color:hsl(0, 0%, 40%);font-size:14px;margin-left:20px;padding-bottom:6px;padding-top:6px}.header_f15ej8wx{-ms-flex-direction:column;-ms-flex-negative:0;-webkit-flex-direction:column;-webkit-flex-shrink:0;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-basis:auto;flex-direction:column;flex-shrink:0}.topHeaderRow_fmhimpa{background:hsl(0, 0%, 90%)}.headerRow_f1jl748l{-ms-flex-direction:row;-ms-flex-negative:0;-webkit-flex-direction:row;-webkit-flex-shrink:0;align-items:flex-end;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-basis:auto;flex-direction:row;flex-shrink:0;flex-wrap:nowrap;max-width:100%;overflow:hidden;padding-left:15px;padding-right:15px}.headerRow_f1jl748l>*{margin-right:10px !important}.headerRow_f1jl748l>*:last-child{margin-right:0px !important}.headerItem_fu8qgk4{-ms-flex-negative:0;-webkit-flex-shrink:0;flex-basis:auto;flex-shrink:0}.headerItem_fu8qgk4 a{color:hsl(0, 0%, 0%);text-decoration:none;text-decoration-color:hsl(0, 0%, 90%)}.headerItem_fu8qgk4 a:hover{background-color:hsl(0, 0%, 90%)}@media (max-width: 690px){.section_f1nupnq0{display:none}}.homeImage_f1fr4cyt{-ms-flex-direction:row;-ms-flex-negative:0;-webkit-flex-direction:row;-webkit-flex-shrink:0;align-items:center;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-basis:auto;flex-direction:row;flex-shrink:0}.homeImage_f1fr4cyt img{max-height:30px}.divider_fiw0lep,.contentPadding_fiw0lep{-ms-flex:1;-webkit-flex:1;flex:1}html{font-size:16px;line-height:1.5}html a{color:hsl(0, 0%, 40%);text-decoration-color:hsl(0, 0%, 40%)}html, body{height:100%;margin:0;padding:0;width:100%}html{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}*,*:before,*:after{box-sizing:inherit}#root{height:100%;width:100%}.page_fghohlb{-ms-flex-direction:column;-webkit-flex-direction:column;display:-ms-flexbox;display:-webkit-flex;display:flex;flex-direction:column}.contentContainer_f16f73v0{-ms-flex:1;-ms-flex-direction:row;-webkit-flex:1;-webkit-flex-direction:row;display:-ms-flexbox;display:-webkit-flex;display:flex;flex:1;flex-direction:row}.content_f14hn0h0{-ms-flex-negative:0;-webkit-flex-shrink:0;flex-basis:auto;flex-shrink:0}@media (min-width: 741px){.content_f14hn0h0{max-width:720px}}@media (max-width: 740px){.content_f14hn0h0{padding-left:10px;padding-right:10px;width:100%}}</style>
  <style>
    html { font-family: 'Roboto', sans-serif; }
  </style>
  <link rel="icon" href="./black-rectangle.svg">
  <meta name="viewport" content="width=device-width">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono" rel="stylesheet">
</head>
<body>
  <div id="root">
    <div class="page_fghohlb"><div class="header_f15ej8wx"><div class="headerRow_f1jl748l topHeaderRow_fmhimpa"><a class="homeImage_f1fr4cyt" href="index.html"><div><?xml version="1.0" encoding="utf-8"?>
<svg width="9px" height="16px" viewBox="0 0 9 16" id="emoji" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <rect x="0" y="0" width="9" height="16" fill="rgb(0, 0, 0)"/>
</svg>
</div></a><div class="divider_fiw0lep"></div><div class="headerItem_fu8qgk4 section_f1nupnq0"><a class="pageLink_f45h" href="885b5a92-7ab7-4ba3-bf51-743934e5af8f.html"><span>tech + education</span></a></div><div class="headerItem_fu8qgk4 section_f1nupnq0"><a class="pageLink_f45h" href="a6c8decc-6e75-443c-aeb6-b0341452138b.html"><span>climbing + training</span></a></div><div class="headerItem_fu8qgk4 section_f1nupnq0"><a class="pageLink_f45h" href="0257c57a-3246-47bc-84db-1438ccac45e6.html"><span>random</span></a></div><div class="headerItem_fu8qgk4 section_f1nupnq0"><a class="pageLink_f45h" href="b193d2d6-f9e9-4a2b-b63e-33ef69b18464.html"><span>about me + contact</span></a></div><div class="headerItem_fu8qgk4 subscribe_f45h">| <a href="/rss.xml">rss</a> <a href="/atom.xml">atom</a> <a href="/buttondown.html">newsletter</a></div></div><div class="headerRow_f1jl748l"><div class="headerItem_fu8qgk4 breadcrumb_f45h"><a class="pageLink_f45h" href="885b5a92-7ab7-4ba3-bf51-743934e5af8f.html"><span>tech + education</span></a></div><div class="headerItem_fu8qgk4 breadcrumb_f45h"> &gt; <a class="pageLink_f45h" href="fd774f7e-5f1d-4d60-8a92-9b0c29d9f046.html"><span>Docker gotchas</span></a></div></div></div><div class="contentContainer_f16f73v0"><div class="contentPadding_fiw0lep"></div><div class="content_f14hn0h0"><h1><div style="white-space:pre-wrap;word-break:break-word"><span>Docker gotchas</span></div></h1><div class="image_f1xdg44s"><img src="images%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F7244b4ff-6bc3-4f08-b072-8854e76062a7_1024x1024.png"/><div class="imageCaption_f1jz7e68"><div style="white-space:pre-wrap;word-break:break-word"></div></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Over the past few months I’ve been transitioning an application from an EC2 environment, managed via Packer / AMI / CodeDeploy to a Docker application running on ECS.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>A big part of this was porting our integration tests to run against the same container as would be deployed to production.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I ran into several gotchas while working on this project, that gave me some interesting insights into what it’s like to use Docker in 2023.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Inspired by Julia Evans’ </span><a href="https://jvns.ca/blog/2023/07/28/why-is-dns-still-hard-to-learn/"><span>blog</span></a><span> about the value of gotchas, I want to share them with you here.</span></div></div><h1><div style="white-space:pre-wrap;word-break:break-word"><span>Docker Desktop for OS X Virtualization</span></div></h1><div class="image_f1xdg44s"><img src="images%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fb40ca29a-7657-404b-b951-755ab52e94bd_1024x1024.png"/><div class="imageCaption_f1jz7e68"><div style="white-space:pre-wrap;word-break:break-word"></div></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>My main encounter with this was that the official image for </span><a href="https://pptr.dev/guides/docker"><span>puppeteer</span></a><span> (a library for controlling headless chrome) did not run successfully on my M1 Docker for Mac, while the same image worked fine on the linux box running tests during our Continuous Integration (CI) workflows.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>After a lot of stumbling around I finally ended up wrapping my head around the issue.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Containers are essentially </span><a href="https://jvns.ca/blog/2016/10/10/what-even-is-a-container/"><span>linux namespaces</span></a><span>, and must be run on a linux host. On OS X, that means a virtual linux. So Docker Desktop for Mac </span><a href="https://www.docker.com/blog/the-magic-behind-the-scenes-of-docker-desktop/"><span>runs a Linux VM</span></a><span>, and the containers actually run on that.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Another layer of complexity is that I have an M1 Mac. A few years ago Apple started releasing </span><a href="https://en.wikipedia.org/wiki/Apple_silicon"><span>machines with ARM processors</span></a><span>. An ARM processor uses a s</span><a href="https://www.redhat.com/en/topics/linux/ARM-vs-x86"><span>maller set of instructions</span></a><span> than an x86 processor to improve energy use and heat.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>A different instruction set requires a different binary - both for the host linux operating system and the containers that you intend to run inside of it. So on my M1 Mac, Docker Desktop actually runs an ARM linux VM, and inside of that can run ARM containers.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>However, many applications (including the </span><a href="https://github.com/puppeteer/puppeteer/issues/7740"><span>headless chrome that puppeteer ships with by default</span></a><span>) have not been compiled for ARM architecture.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>But all is not lost! The linux ARM VM can actually emulate an x86 architecture. Previously this was done using QEMU. Apple developed a different emulation framework called Rosetta 2, and Docker Desktop currently (Aug 2023) comes with a beta feature that allows you to use it instead. (</span><a href="https://spin.atomicobject.com/2023/03/22/docker-apple-silicon/"><span>a nice explanation of this</span></a><span>).</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>For some reason, QEMU isn’t able to successfully emulate headless chrome, but </span><a href="https://github.com/puppeteer/puppeteer/issues/7746#issuecomment-1382021497"><span>Rosetta 2 seems to do it</span></a><span>. However, Rosetta 2 crashes when compiling my application, leading to an annoying little dance that I have to do every time I need to rebuild my application (turn off Rosetta 2, build my app, turn it back on, run puppeteer).</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>It seems like it should be possible to figure out how to run headless chrome (or chromium) in an ARM container. I have not tried to shave this particular yak yet. Also, it would mean running tests using a different browser locally vs in CI. (Though one could argue that the browsers are already effectively quite different due to all the emulation).</span></div></div><h2><div style="white-space:pre-wrap;word-break:break-word"><span>Bonus: a remote linux host for docker development</span></div></h2><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>After getting whacked by one-too-many rakes, I opted to spin up a remote x86 linux machine to test out my docker containers.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I first tried using github’s codespaces for this, since I assumed it would make for a nice out-of-the-box experience for working with my github repo. However, my app needed to fetch some private packages via git+ssh from github, and github codespaces does </span><a href="https://github.com/orgs/community/discussions/12711#discussioncomment-6569350"><span>not support this out of the box</span></a><span>. Go figure.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I ended up going with EC2. I set up an ssh key, punched a hole in the security group for my IP, and I was off to the races. When I sshed into the machine, ssh-agent would forward my credentials to the machine, and then I was able to forward them into the container by using an </span><a href="https://docs.docker.com/engine/reference/commandline/buildx_build/#ssh"><span>ssh mount</span></a><span>.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>My workflow then consisted of making changes on my local machine, </span><a href="https://linux.die.net/man/1/rsync"><span>rsync</span></a><span>-ing the changes to the EC2 host, and then running my test scripts via ssh.</span></div></div><h1><div style="white-space:pre-wrap;word-break:break-word"><span>Max number of bridge networks</span></div></h1><div class="image_f1xdg44s"><img src="images%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F57ef3dd4-ff34-4bd6-86d4-af33584466d3_1024x1024.png"/><div class="imageCaption_f1jz7e68"><div style="white-space:pre-wrap;word-break:break-word"></div></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I needed to bring up multiple instances of my application, so I could run tests in parallel against them. I wanted each instance to have a dedicated db, so it wouldn’t be possible for tests to interfere with each-other’s data. I opted to do this using docker compose. I set up a compose file consisting of a db container, an app container, and a puppeteer container. I then used the </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`—project-name`</span><span> flag to launch multiple copies of this 3-container project.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Running on a 72-core codebuild instance, I tried to run 72 projects like this. However, I quickly found that only 31 of the projects were starting successfully. The rest were failing with:</span></div></div><div>code<!-- --> not implemented</div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>It turns out that by default, docker compose will create a new </span><a href="https://docs.docker.com/network/drivers/bridge/"><span>bridge network</span></a><span> for each project. The docker engine has a certain private IP address space available to it on the host machine. It carves it up into segments of a fixed size. Each bridge network is assigned a unique segment, and docker ensures that containers in different segments cannot communicate with each other.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>The default settings choose subnets that are quite large, allowing for many containers to be launched inside each bridge network. However, using subnets of this size means that only 31 bridge networks can be created before the host’s address space is exhausted (with the default bridge network that’s 32 total). Thankfully there’s now an </span><a href="https://github.com/moby/moby/pull/29376"><span>option for this</span></a><span>.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Launching dockerd with this flag fixed the issue for me:</span></div></div><div>code<!-- --> not implemented</div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>This sets the size of the subnet to be smaller, so fewer containers can be launched in each bridge network, but it allows up to 256 bridge networks. Since I’m only ever running 3 containers per network, this works just fine for me.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Thanks for these wonderful bloggers for helping me figure this out: </span><a href="https://straz.to/2021-09-08-docker-address-pools/"><span>blog1</span></a><span> </span><a href="https://bitesizedevtips.com/blog/docker-too-many-networks/"><span>blog2</span></a></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>Another option would be to configure compose to </span><a href="https://docs.docker.com/compose/compose-file/compose-file-v3/#network_mode"><span>use the host’s network</span></a><span> for all the services, which would also remove the isolation between different projects.</span></div></div><h1><div style="white-space:pre-wrap;word-break:break-word"><span>File permissions when bind-mounting volumes.</span></div></h1><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>When the tests run, they create a bunch of files - test results, video recordings, log files. These files are useful when you need to debug a failing build. However, these files live inside of the containers. I wanted the files on the host instead, so I could upload them to s3 at the end of the build. I also wanted this to be independent of container lifecycles. Even if a container crashed or was removed, I still wanted the files to persist to help debug what went wrong.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I solved this problem by bind-mounting a </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`test_output/worker_&lt;idx&gt;`</span><span> host directory into a </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`test_output`</span><span> directory in each worker. The worker would write into its own </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`test_output`</span><span> directory, and the files would all end up in the </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`test_output/worker_&lt;idx&gt;`</span><span> directories on the host. Then I could just sync that whole directory to s3.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>One thing that I didn’t really anticipate is that when bind-mounting a directory, it exists both in the host and the container. Each of those environments is its own OS, with potentially different users and permissions.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>It seems that on linux, Docker opts to do the simple thing, which is to not mess with the permissions. So if a file is owned by uid 1001 on the host machine, it will be owned by the same uid inside the container, even if that uid doesn’t exist. Similarly, if your container runs as the root user and creates a file in a bind-mounted volume, the file will be owned by the root user in the host machine. </span><a href="https://techflare.blog/permission-problems-in-bind-mount-in-docker-volume/"><span>This article</span></a><span> provides a more thorough explanation and suggests some ways to work around this.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>In Docker Desktop for OS X, the situation is </span><a href="https://github.com/docker/for-mac/issues/6243"><span>more complicated</span></a><span>, owing to the extra layer of virtualization, and several options for how the OS X filesystem is mapped to the linux VM.</span></div></div><h1><div style="white-space:pre-wrap;word-break:break-word"><span>Logging and disappearing containers.</span></div></h1><div class="image_f1xdg44s"><img src="images%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Faefc7b42-0d71-4a0b-b12f-be1c1cbe0d21_1024x1024.png"/><div class="imageCaption_f1jz7e68"><div style="white-space:pre-wrap;word-break:break-word"></div></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>When you launch a command in a container, using the CMD or ENTRYPOINT instruction, docker will save the stdout and stderr streams for that command. You can then access those logs using the </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`docker logs`</span><span> command.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I was trying to also persist these logs, and so I attempted to do something like this:</span></div></div><div>numbered_list_item<!-- --> not implemented</div><div>numbered_list_item<!-- --> not implemented</div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>At some point some of my containers were failing and so I went to check the service.log files, and they were all empty.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>It turns out that by default, the </span><a href="https://sematext.com/blog/docker-logs-location/"><span>log lifecycle is linked with the container lifecycle</span></a><span>, so the log only exists while the container exists.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>In my compose file I was using </span><a href="https://docs.docker.com/compose/compose-file/05-services/#depends_on"><span>depends_on</span></a><span> clauses to make my app container wait until the db container was healthy before starting up. That means that when I was running the log command, the container did not yet exist, so the log command would fail.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I tried waiting until all the containers were up and trying the log command then. However, when a container runs into an error, docker compose will automatically remove the container. So by the time I try to get the logs, the container (and the logs) are already gone.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I’m not proud of this, but I ended up hacking around this issue by continuously polling the </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`docker compose logs —follow`</span><span> command after the issuing the compose up command. Eventually the container would come up, and the command would succeed and tail the logs from that point on.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>There is still a possibility that the container will start, encounter an error, and be removed between polls. However, this solution seems good enough for now.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>I think a cleaner solution would probably be to just have each container write to a log file in its bind-mounted </span><span style="font-family:&#x27;Roboto Mono&#x27;, monospace;">`test_output`</span><span> directory, instead of stdout/stderr.</span></div></div><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>It should also be possible to do this by using a </span><a href="https://docs.docker.com/config/containers/logging/syslog/"><span>different logging driver</span></a><span>.</span></div></div><h1><div style="white-space:pre-wrap;word-break:break-word"><span>The End</span></div></h1><div class="paragraph_f130nt8a"><div style="white-space:pre-wrap;word-break:break-word"><span>These were the highlights of my journey into Docker. It was an illuminating, although not entirely pleasant experience. I hope it saves someone out there some trouble.</span></div></div></div><div class="contentPadding_fiw0lep"></div></div></div>
  </div>
</body>
</html>